{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Sequence\n",
    "import functools\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the object passed between nodes\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[AIMessage]\n",
    "    sender: str\n",
    "    user_query: str  # Add user_query to the AgentState\n",
    "\n",
    "# Tool Initialization for the Researcher\n",
    "tavily_tool = TavilySearchResults(max_results=5)  # Tool to fetch search results from the internet\n",
    "\n",
    "# WatsonxLLM parameters\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "\n",
    "# Load API key and project ID from environment variables\n",
    "watsonx_api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"  # Replace with your region's URL\n",
    "\n",
    "if not watsonx_api_key:\n",
    "    raise ValueError(\"Please set the WATSONX_API_KEY in your .env file.\")\n",
    "if not project_id:\n",
    "    raise ValueError(\"Please set the PROJECT_ID in your .env file.\")\n",
    "\n",
    "# Define LLM models using WatsonxLLM\n",
    "llm_creator = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # Choose your desired model\n",
    "    url=url,\n",
    "    apikey=watsonx_api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "llm_router = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # Choose your desired model\n",
    "    url=url,\n",
    "    apikey=watsonx_api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Router Agent Node (modified)\n",
    "def router_agent_node(state, name):\n",
    "    \"\"\"Router agent that selects either 'Researcher' or 'Creator' based on the message.\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "\n",
    "    # Router prompt that instructs the LLM to choose the appropriate agent\n",
    "    router_prompt = (\n",
    "        \"You are a routing agent. Your task is to select one of two agents based on the user input.\\n\"\n",
    "        \"If the user query is about recent information or data like prices, select 'Researcher'.\\n\"\n",
    "        \"If the user query is more general or requires a knowledge-based response, select 'Creator'.\\n\\n\"\n",
    "        f\"User Query: {query}\\n\\n\"\n",
    "        \"Answer with only one word: 'Researcher' or 'Creator'.\"\n",
    "    )\n",
    "\n",
    "    # Generate the response using the LLM\n",
    "    selected_agent = llm_router.predict(router_prompt).strip().lower()\n",
    "\n",
    "    # Normalize the output to either 'Researcher' or 'Creator'\n",
    "    if 'researcher' in selected_agent:\n",
    "        selected_agent = 'Researcher'\n",
    "    elif 'creator' in selected_agent:\n",
    "        selected_agent = 'Creator'\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected agent response: {selected_agent}\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Router Agent: Selected {selected_agent}\", name=name)],\n",
    "        \"sender\": name,\n",
    "        \"selected_agent\": selected_agent,  # Include the selected agent in the state\n",
    "        \"user_query\": query,  # Store the user query in the state\n",
    "    }\n",
    "\n",
    "# Researcher Agent Node\n",
    "def researcher_agent_node(state, name):\n",
    "    \"\"\"Researcher agent that fetches data from the internet.\"\"\"\n",
    "    query = state[\"user_query\"]  # Access user_query from the state\n",
    "    search_results = tavily_tool.invoke(query)  # Perform search\n",
    "    result = f\"Researcher Agent: Fetched search results for '{query}': {search_results}\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result, name=name)],\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "# Creator Agent Node (LLM)\n",
    "def creator_agent_node(state, name):\n",
    "    \"\"\"Creator agent that uses a language model to generate responses.\"\"\"\n",
    "    query = state[\"user_query\"]  # Access user_query from the state\n",
    "    # Include the user query in the prompt for the Creator\n",
    "    creator_prompt = f\"Please answer the following query: {query}\"\n",
    "    result = llm_creator.predict(creator_prompt)  # Generate response using the LLM\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Creator Agent: {result}\", name=name)],\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "# Define nodes for each agent\n",
    "research_node = functools.partial(researcher_agent_node, name=\"Researcher\")\n",
    "creator_node = functools.partial(creator_agent_node, name=\"Creator\")\n",
    "\n",
    "# Define the graph and its nodes\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Router\", functools.partial(router_agent_node, name=\"Router\"))\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Creator\", creator_node)\n",
    "\n",
    "# Routing logic based on Router Agent's decision\n",
    "def extract_selected_agent(state) -> str:\n",
    "    \"\"\"Extracts the selected agent ('Researcher' or 'Creator') from the Router agent's response.\"\"\"\n",
    "    return state[\"selected_agent\"]\n",
    "\n",
    "# Add conditional edges based on the Router agent's response\n",
    "workflow.add_conditional_edges(\"Router\", extract_selected_agent, {\"Researcher\": \"Researcher\", \"Creator\": \"Creator\"})\n",
    "\n",
    "# After either Researcher or Creator is done, the program ends\n",
    "workflow.add_conditional_edges(\"Researcher\", lambda state: \"__end__\", {\"__end__\": END})\n",
    "workflow.add_conditional_edges(\"Creator\", lambda state: \"__end__\", {\"__end__\": END})\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"Router\")\n",
    "\n",
    "# Compile the workflow for execution\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Function to display the architecture\n",
    "def display_architecture():\n",
    "    \"\"\"Display the graph architecture.\"\"\"\n",
    "    try:\n",
    "        display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "    except:\n",
    "        print(\"Unable to display graph architecture. Extra dependencies might be missing.\")\n",
    "\n",
    "# Function to evaluate a message and run the system\n",
    "def evaluate_message(message: str):\n",
    "    \"\"\"Evaluate the message and route it through the system.\"\"\"\n",
    "    try:\n",
    "        events = graph.stream(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=message)\n",
    "                ]\n",
    "            },\n",
    "            {\"recursion_limit\": 50}\n",
    "        )\n",
    "\n",
    "        for s in events:\n",
    "            print(s)\n",
    "            print(\"----\")\n",
    "        print(\"Final\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# Display the architecture\n",
    "display_architecture()\n",
    "\n",
    "# Example call to test the system\n",
    "evaluate_message(\"Fetch the bitcoin price over the past 5 days.\")  # Researcher case\n",
    "evaluate_message(\"Explain what Bitcoin is.\")  # Creator case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
